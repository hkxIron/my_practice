{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大规模 Transformer 模型 8 比特量化矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes\n",
    "# blog https://hf-mirror.com/blog/hf-bitsandbytes-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X=torch.Tensor([[2, -1, -1],\n",
    "\t[0,3,2],\n",
    "\t[-1,-1,0]\n",
    "])\n",
    "\n",
    "W=torch.Tensor([[-1, 0],\n",
    "\t[0, -2],\n",
    "    [-1, 2]])\n",
    "Cx=X.abs().max(axis=1).values  # 找出每列的绝对值的最大值\n",
    "Cw=W.abs().max(axis=0).values  # 找出每行的绝对值的最大值\n",
    "\n",
    "X_i8=(X*(127/Cx.view(3,1))).to(torch.int8)\n",
    "W_i8 =(W*(127/Cw.view(1,2))).to(torch.int8)\n",
    "Out_i32=X_i8.to(torch.int32)@W_i8.to(torch.int32)\n",
    "Out_f16=Out_i32*(Cx.view(3,1)/127)*(Cw.view(1,2)/127) # 先对结果进行逐列的norm，再对结果进行逐行的norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 127,  -63,  -63],\n",
       "        [   0,  127,   84],\n",
       "        [-127, -127,    0]], dtype=torch.int8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-127,    0],\n",
       "        [   0, -127],\n",
       "        [-127,  127]], dtype=torch.int8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_i8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8128,      0],\n",
       "        [-10668,  -5461],\n",
       "        [ 16129,  16129]], dtype=torch.int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Out_i32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0079,  0.0000],\n",
       "        [-1.9843, -2.0315],\n",
       "        [ 1.0000,  2.0000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Out_f16  # int8量化的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out_origin=X@W # 原始float32的相乘结果，可以看到还是有一定的差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [-2., -2.],\n",
       "        [ 1.,  2.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Out_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0155)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Out_f16-Out_origin).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
